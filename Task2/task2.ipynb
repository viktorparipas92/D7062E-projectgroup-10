{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# D7062E Project\n",
    "## Project Group 10\n",
    "\n",
    "Contributors:\n",
    "- Theo HEMBÄCK\n",
    "- PARIPÁS Viktor\n",
    "- Jerker ÅBERG\n",
    "- Kristofer ÅGREN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing dependencies\n",
    "We used the following libraries in the first task:\n",
    "- `pandas` to manipulate the data\n",
    "- `scikit-learn` for imputing and scaling the data\n",
    "- `seaborn` and `matplotlib` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data exploration\n",
    "Let's load the training dataset from the corresponding .csv file.\n",
    "Since we know that the columns represent the mean/standard deviation of the positions and angles of the 60 points, respectively, followed by the label name and code, let us rename the columns accordingly to allow for easier reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  training_data = pd.read_csv('train-final.csv', header=None)\n",
    "  test_data = pd.read_csv('test-final.csv', header=None)\n",
    "\n",
    "  name_mappings = {\n",
    "      # Feature columns\n",
    "      **{i:f'positions_mean_{i}' for i in range(60)},\n",
    "      **{i:f'positions_std_{i}' for i in range(60, 120)},\n",
    "      **{i:f'angles_mean_{i}' for i in range(120, 180)},\n",
    "      **{i:f'angles_std_{i}' for i in range(180, 240)},\n",
    "      # Label columns\n",
    "      **{240: 'label_name', 241: 'label_code'},\n",
    "  }\n",
    "\n",
    "  training_data.rename(name_mappings, axis=1, inplace=True)\n",
    "  training_feature_columns = training_data.columns[:-2]\n",
    "\n",
    "  training_features = training_data[training_feature_columns]\n",
    "  training_labels = training_data.label_name\n",
    "  training_codes = training_data.label_code\n",
    "\n",
    "  test_data.rename(name_mappings, axis=1, inplace=True)\n",
    "  test_feature_columns = test_data.columns[:-2]\n",
    "\n",
    "  test_features = test_data[test_feature_columns]\n",
    "  test_labels = test_data.label_name\n",
    "\n",
    "  return training_features, training_labels, training_codes, test_features, test_labels\n",
    "\n",
    "\n",
    "training_features, training_labels, training_codes, test_features, test_labels = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's show some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How many different labels do we have in the training dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "number_of_classes = training_labels.nunique()\n",
    "number_of_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's take a look at how many occurrences we have of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_labels.value_counts().plot(kind='bar', figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that `child` is the most common label in the training dataset and that `go` is the least common label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's look for columns that have missing values. The missing values are in the following columns (along with the missing value count):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Look for columns that have missing values\n",
    "columns_null_sum = training_features.isnull().sum()\n",
    "columns_with_nulls = columns_null_sum[columns_null_sum > 0]\n",
    "\n",
    "print(\n",
    "    \"Total amount of missing values in the dataframe:\", \n",
    "    training_features.isnull().sum().sum()\n",
    ")\n",
    "print(\n",
    "    \"Missing values in the following column indexes (and missing value count):\"\n",
    ")\n",
    "print(columns_with_nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Some classifiers are more sensitive to the range, mean & outliers of the features, such as linear regression models, for example.\n",
    "In order to be able to train a wide range of classifiers and compare them, we will need to preprocess the data for scaling and outlier treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see if the dataset also contains outliers. There are quite a few way to detect outliers (Source:\n",
    "[Outlier detection methods in Scikit-Learn](https://scikit-learn.org/stable/modules/outlier_detection.html)):\n",
    "- Isolation forest\n",
    "- Local outlier factor\n",
    "- One-class support vector machine (SVM)\n",
    "- Elliptic envelope\n",
    "\n",
    "We start by doing a boxplot for all features to get a visual indication of the outlier situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_features.boxplot(figsize=(18,7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the boxplot, there appears to be many columns with outliers. Many classifiers, e.g. linear classifiers like Logistic Regression will not handle outliers well, so we need to find a way to handle also outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Outliers\n",
    "\n",
    "As we saw in the boxplot above, there are many columns with outliers. And while there are many methods to detect outliers,let's begin with just identifying the **values** that are farthest from the mean. \n",
    "\n",
    "A simple approach is to identify the values that lie outside of 3$\\sigma$ (that is, three times the standard deviation) as outliers, and drop the rows that have at least one outlier. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#training_features_outliers_marked = training_features[abs(training_features) <= 3]\n",
    "from scipy import stats\n",
    "\n",
    "training_features_outliers_marked = training_features[\n",
    "    np.abs(stats.zscore(training_features.fillna(training_features.mean()))) < 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_features_outliers_marked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_features_outliers_removed = training_features_outliers_marked.dropna()\n",
    "training_features_outliers_removed.boxplot(figsize=(18,7))\n",
    "plt.xticks([1], [''])\n",
    "print(\"Number of rows left\", training_features_outliers_removed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The boxplot now looks better, except for the second part (columns 60 to 120), which is `positions_std_i`.\n",
    "We can also see that if we remove all the rows with at least one detected outlier, we are left with less than half of the original data! This is due to the large number of features.\n",
    "\n",
    "We need another method for this dataset, let's instead cap the outliers to 3 sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline_outliers(df, std_cap=3):\n",
    "  df = df.copy()\n",
    "\n",
    "  for column in df.columns:\n",
    "\n",
    "    mean = df[column].mean(skipna = True)\n",
    "    std = df[column].std(skipna = True)\n",
    "    \n",
    "    df[column] = np.clip(df[column], -(mean + std_cap*std), mean + std_cap*std)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pipeline_outliers(training_features)\n",
    "df.boxplot(figsize=(18,7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we saw above, there are 6 columns that have missing values (3 or 4 missing values each). Many classifiers do not handle missing values directly, such as Logistic Regression and SVM, for example. As such we need to find a way to manage the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are many different ways of handling missing values and we will explore a few of them here. To get started, let's examine the features/columns that contain missing data. The two visualizations chosen for each of the features/columns are:\n",
    "\n",
    "- *Histogram* - This will give a good indication of the distribution, for example if it appears to be normal.\n",
    "- *Boxplot* - We get some additional information from the boxplot showing the median, quartiles as well as outliers.\n",
    "\n",
    "Let's plot the distributions for the columns with missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_distributions_for_columns(dataframe, columns):\n",
    "    # Plot distributions for each of the columns that have missing values\n",
    "    figure, axes = plt.subplots(2, len(columns), figsize=(12, 6))\n",
    "\n",
    "    for index, column in enumerate(columns):\n",
    "        # plot a histogram of the column for the first row\n",
    "        dataframe[column].plot(\n",
    "            kind='hist', ax=axes[0, index], title=column, bins=15\n",
    "        )\n",
    "        # Do a box plot as well\n",
    "        sns.boxplot(y=dataframe[column], ax=axes[1, index]).set_title(column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_distributions_for_columns(training_features, columns_with_nulls.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The distributions are quite different, which mean we may need different imputation techniques for each column.\n",
    "For example, the columns `positions_mean_9` and `positions_mean_15` are clearly not normal distributions and replacing missing values with the mean would likely not be ideal.\n",
    "For instance in the case of `positions_mean_9` the mean is close to 0 where few other samples are, moreover this feature may even need to be split into two separate features as it appears to be the combination of two gaussian distributions.\n",
    "\n",
    "Also, we do not know if the missing values themselves have a significance, i.e. we might want to create a separate column to indicate that a missing value is present or not. There are relatively few rows that have missing values, though, that may limit the usefulness of this technique and usefulness will also depend on the classifier used in the end.\n",
    "\n",
    "For now, we will implement support for the following imputation strategies:\n",
    "- Replacing with **mean**\n",
    "- **Drop the rows** containing at least one missing value\n",
    "- **K-Nearest Neighbour (KNN)** imputation, i.e. use the mean value of the K nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "def impute(dataframe, imputer_class, **kwargs):\n",
    "    imputer = imputer_class(**kwargs)\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[dataframe.columns] = imputer.fit_transform(dataframe.values)\n",
    "    return dataframe\n",
    "\n",
    "# KNN imputation\n",
    "def impute_knn(dataframe):\n",
    "    return impute(dataframe, KNNImputer, n_neighbors=2, weights='uniform')\n",
    "\n",
    "# Drop rows\n",
    "def impute_drop_rows(dataframe):\n",
    "    return dataframe.dropna()\n",
    "\n",
    "# Mean imputation\n",
    "def impute_mean(dataframe):\n",
    "    return impute(\n",
    "        dataframe, SimpleImputer, missing_values=np.nan, strategy='mean'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's compare the mean and KNN imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns_null_sum = training_features.isnull().sum()\n",
    "columns_with_nulls = columns_null_sum[columns_null_sum > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#training_features = training_data[training_feature_columns]\n",
    "all_na_values = training_features.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First let's execute both imputations individually.\n",
    "Starting with the mean imputation, the imputed values are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data_mean_imputed = impute_mean(training_features)\n",
    "training_data_mean_imputed.values[all_na_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While the KNN-imputed values look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data_knn_imputed = impute_knn(training_features)\n",
    "training_data_knn_imputed.values[all_na_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparison of mean and KNN\n",
    "Now it is time to compare the results and visualize the differences on a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "differences = (\n",
    "    training_data_mean_imputed.values[all_na_values]\n",
    "    - training_data_knn_imputed.values[all_na_values]\n",
    ")\n",
    "plt.hist(differences);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can conclude that the differences between the methods are not significant, the mode is close to zero, for most of the missing values the two methods give very similar imputed values - more than half of the values are in the range [-0.1,0.1]\n",
    "\n",
    "Now let's look at how the difference is distributed for each column using a boxplot and a swarm plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "na_rows, na_columns = np.where(all_na_values)\n",
    "dataframe_differences_columns = pd.DataFrame(\n",
    "    {'diff': differences, 'column': training_features.columns[na_columns]}\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "\n",
    "axis = sns.boxplot(\n",
    "    x='column', data=dataframe_differences_columns, y='diff', ax=axs[0]\n",
    ")\n",
    "axis.set_xticklabels(axis.get_xticklabels(), rotation=90);\n",
    "\n",
    "axis = sns.swarmplot(\n",
    "    x='column', data=dataframe_differences_columns, y='diff', ax=axs[1]\n",
    ")\n",
    "axis.set_xticklabels(axis.get_xticklabels(), rotation=90);\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When comparing the imputed values between the 'mean' and 'KNN' approaches, we see that they produce very similar values for columns `positions_mean_7` and `positions_mean_16` but larger differences by varying degrees for the other columns. The largest (absolute) deviance is for `positions_mean_9`.\n",
    "\n",
    "\n",
    "Based on this it is likely that we may need to employ different imputation techniques depending on the column/feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For now, however, let's use KNN imputation and make sure that after the imputation there are no more missing values in our dataset. We will bring these three different methods into Task 2 of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a pipeline function that uses the KNN imputation\n",
    "def pipeline_missing_values(df, method = \"knn\"):\n",
    "  df = df.copy()\n",
    "  \n",
    "  columns_null_sum = df.isnull().sum()\n",
    "  columns_with_nulls = columns_null_sum[columns_null_sum > 0]\n",
    "\n",
    "  if method == \"knn\":\n",
    "    df[columns_with_nulls.index] = impute_knn(df[columns_with_nulls.index])\n",
    "  elif method == \"mean\":\n",
    "    df[columns_with_nulls.index] = impute_mean(df[columns_with_nulls.index])\n",
    "  else:\n",
    "    raise f\"Unknown method {method}\"\n",
    "\n",
    "  assert df.isnull().sum().sum() == 0\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scaling\n",
    "First we will take a look at how the data looks by feature/column. While the dataset contains a lot of features, we can use a boxplot to get an overview understanding of how the different columns compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's make a boxplot for every feature to get an overview of how they all relate in terms of range & centre. No need to have labels for the feature names, we just want to show all of them in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_features.boxplot(figsize=(18, 7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The columns are clearly not all scaled to the same range and they have different means. As such a scaler that centers the mean and normalizes (scale to the variance) may be suitable. Let's use scikit's `StandardScaler` for this.\n",
    "\n",
    "The `StandardScaler`normalizes the data so that the mean becomes zero, and the variance one, i.e. the scaled dataset follows a *standard* normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline_scale(dataframe):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(dataframe.values)\n",
    "    return pd.DataFrame(scaled_values)\n",
    "\n",
    "\n",
    "training_features_scaled = pipeline_scale(training_features)\n",
    "training_features_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looks good, i.e. the mean is about 0 and the standard deviation is around 1. All the columns have now been scaled. Let's rerun the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_features_scaled.boxplot(figsize=(18, 7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the Methods section above we have defined the pipeline methods needed for handling outliers, missing data and scaling. Let's put them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "training_features, training_labels, training_codes, test_features, test_labels = load_data()\n",
    "\n",
    "# Plot before pipeline\n",
    "training_features.boxplot(figsize=(18, 7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_features.boxplot(figsize=(18, 7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run the pipeline and rerun the boxplot on the resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_data_pipeline(features, std_cap = 3, impute_method = \"knn\"):\n",
    "  df = pipeline_outliers(features, std_cap=std_cap)\n",
    "  df = pipeline_missing_values(df, method = impute_method)\n",
    "  df = pipeline_scale(df)\n",
    "\n",
    "  return df\n",
    "\n",
    "train_df = run_data_pipeline(training_features)\n",
    "train_df.boxplot(figsize=(18, 7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_df = run_data_pipeline(test_features)\n",
    "\n",
    "test_df.boxplot(figsize=(18, 7))\n",
    "plt.xticks([1], [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following observations have been made of the dataset and affects how models should be trained: \n",
    "\n",
    "- There are low number, less than 30, of samples per class. As such, overfitting will likely be a problem with more advanced classifiers like decision trees, for example. To address this, we will use a Cross Validation method to evaluate the fit. Furthermore, to ensure we have the same distribution in each fold of the CV, a stratified fold method will be used.\n",
    "- The data set is very 'wide' - that is there are a very large amount of features (240) compared to the number of samples (540). To address this, a feature reduction will be used. There are many options for reducing features, such as removing features with strong correlation, low variance or fitting a classifier and removing those features that have low importance. Dimensionality reduction approaches are also common, such as Principal Component Analysis, for example. \n",
    "- We will compare RFE to PCA, and select the best performing option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RFE \n",
    "\n",
    "To reduce features, we have selected a Recursive Feature Elimination method using a Support Vector Machine classifier. The reason for selecting a SVM classifier is the combination of speed and the ability to capture non-linearities in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import svm\n",
    "\n",
    "train_df = run_data_pipeline(training_features, std_cap = 6, impute_method=\"mean\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def reduce(n_feats):\n",
    "\n",
    "    rfe_selector = RFECV(estimator=svm.SVC(kernel = 'linear'), \n",
    "                        cv = StratifiedKFold(n_splits = 10), \n",
    "                        min_features_to_select = n_feats, step = 1, n_jobs=4)\n",
    "\n",
    "    rfe_selector.fit(train_df, training_labels)\n",
    "\n",
    "    selected_cols = train_df.columns[rfe_selector.get_support()]\n",
    "    score = np.mean(rfe_selector.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "    print(n_feats, len(selected_cols), score)\n",
    "\n",
    "    return selected_cols, score\n",
    "\n",
    "# Evaluate a few different minimum features to select\n",
    "rfe_results = [reduce(n) for n in [15, 45]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As seen above, the cross validated performance peaks at 40 selected features, at around an accuracy of 0.851."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PCA\n",
    "\n",
    "Let's also look at how PCA would perform, also evaluating it using the same SVM classifiers as in the RFE case and same amount of CV folds (10). Code below is based on sklearn example at https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"pca\", pca), (\"svm\", svm.SVC(kernel=\"linear\"))])\n",
    "param_grid = {\n",
    "    \"pca__n_components\": np.arange(10,100,5)\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, cv=10, n_jobs=4)\n",
    "search.fit(train_df, training_labels)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using PCA, we achieve the highest accuracy of 0.804 on the same cross validated classifiers as RFE above using 85 PCA components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "The RFE approach gives us higher accuracy with fewer features than PCA, so we will select the RFE approach instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rfe_best_ix = np.argmin([score for _,score in rfe_results])\n",
    "rfe_columns,_ = rfe_results[rfe_best_ix]\n",
    "\n",
    "train_reduced_df = train_df[train_df.columns[rfe_columns]]\n",
    "test_reduced_df = test_df[test_df.columns[rfe_columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model selection\n",
    "First we get a baseline score for some of the avaliable classifiers in sklearn. The classifiers are imported from sklearn along with classification metrics for evaluation. The baseline scores are calculated using cross validation with 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Training and evaluation on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create a function that trains and evaluates a model for each of the selected classifiers with default hyperparameters and run it on the feature-reduced training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_all_classifiers(train_df):\n",
    "    classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB(),\n",
    "    GradientBoostingClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    MLPClassifier(),\n",
    "    # use dummy classifier to get a baseline\n",
    "    DummyClassifier(strategy=\"most_frequent\")\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    scoring =[\"accuracy\",\"precision_macro\", \"recall_macro\", \"f1_macro\"]\n",
    "\n",
    "    for clf in classifiers:\n",
    "\n",
    "        scores = cross_validate(clf, train_df, training_labels, scoring=scoring, cv=10, return_train_score = True)\n",
    "\n",
    "        print(f\"Classifier: {clf.__class__.__name__}\", \"test accuracy\", np.mean(scores[\"test_accuracy\"]))\n",
    "\n",
    "        scores_mean = {a:np.mean(scores[a]) for a in scores.keys()}\n",
    "        scores_mean[\"classifier\"] = clf.__class__.__name__\n",
    "\n",
    "        # save the evaluation results in a dataframe\n",
    "        all_results.append(pd.DataFrame([scores_mean]))\n",
    "\n",
    "    return pd.concat(all_results)\n",
    "\n",
    "results = evaluate_all_classifiers(train_reduced_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Baseline scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.sort_values(by=['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Conclusion\n",
    "\n",
    "The best performing classifier (using CV with 10 folds) is the `ExtraTreeClassifier` with an accuracy of approximately 0.93."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Fine-tuning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next step is to find optimal hyperparameters on the best performing classifier, `ExtraTreesClassifier`. We will do so by doing a CV Grid Search and will tune the following hyperparameters of the classifier:\n",
    "\n",
    "- n_estimators, controls the number of trees generated. Higher can give better performance, but may also increase likelihood of overfitting. The default value is 100 so we will probe around that value.\n",
    "- max_depth, the maximum depth of the tree. The default value is None, i.e. the max depth is not restricted so we first tested a wide range and concluded around 16 gives good results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': np.arange(95,105,1),\n",
    "               'max_depth': [15,16,17,18],\n",
    "               'bootstrap': [False, True]}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "searchCV = GridSearchCV(estimator=ExtraTreesClassifier(random_state=42), scoring='accuracy', cv=cv, param_grid=param_grid, verbose=True, n_jobs = 4)\n",
    "\n",
    "searchCV.fit(train_reduced_df, training_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "searchCV.best_params_, searchCV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This gives the following best parameters to use in the evaluation on the test set:\n",
    "\n",
    "\n",
    "1.   max_depth: 18\n",
    "2.   n_estimators: 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Evaluation on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Choice of models to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We choose to evaluate the the models with the top five `test_accuracy` from the model selection section:\n",
    "\n",
    "1.   `ExtraTreesClassifier` - 0.928\n",
    "2.   `LogisticRegression` - 0.896\n",
    "3.   `MLPClassifier` - 0.888\n",
    "4.   `SVC` - 0.888\n",
    "5.   `RandomForrestClassifier` - 0.885\n",
    "\n",
    "After these five models the accuracy starts to drop off a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We fit the models to the preprocessed and feature-reduced training set, run predictions on the test set run through the same preprocessing and feature reduction and use `accuracy_score` from `sklearn.metrics` to get the final accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_models(training_features, training_labels, test_features, test_labels, classifiers):\n",
    "    prediction_scores = {}\n",
    "\n",
    "    for index, clf in enumerate(classifiers):\n",
    "      clf.fit(training_features, training_labels)\n",
    "      prediction = clf.predict(test_features)\n",
    "      prediction_scores[clf.__class__.__name__] = (accuracy_score(test_labels, prediction))\n",
    "    return(prediction_scores)\n",
    "\n",
    "class ExtraTreesClassifier_Finetuned(ExtraTreesClassifier):\n",
    "\n",
    "    def __init__(self):\n",
    "      ExtraTreesClassifier.__init__(self, n_estimators=102, max_depth=18, random_state=42)\n",
    "\n",
    "classifiers = [\n",
    "      ExtraTreesClassifier(),\n",
    "      ExtraTreesClassifier_Finetuned(),\n",
    "      LogisticRegression(),\n",
    "      MLPClassifier(),\n",
    "      SVC(),\n",
    "      RandomForestClassifier()\n",
    "      ]\n",
    "\n",
    "prediction_scores = evaluate_models(train_reduced_df, training_labels, test_reduced_df, test_labels, classifiers)\n",
    "prediction_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot the performance (accuracy) of the classifiers on the train vs test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "classifiers = ['ExtraTreesClassifier', \n",
    "               'ExtraTreesClassifier_Finetuned', \n",
    "               'LogisticRegression',\n",
    "               'MLPClassifier',\n",
    "                'SVC',\n",
    "                'RandomForestClassifier']\n",
    "\n",
    "N = len(classifiers)\n",
    "\n",
    "train_results_df = pd.concat([results, pd.DataFrame([{\"classifier\": \"ExtraTreesClassifier_Finetuned\", \n",
    "                                                 \"test_accuracy\" : searchCV.best_score_}])])\n",
    "\n",
    "train_results_df = train_results_df.set_index(\"classifier\")\n",
    "\n",
    "# Specify the values of blue bars (height)\n",
    "blue_bar = train_results_df.loc[classifiers][\"test_accuracy\"].values\n",
    "# Specify the values of orange bars (height)\n",
    "orange_bar = [prediction_scores[c] for c in classifiers]\n",
    "\n",
    "# Position of bars on x-axis\n",
    "ind = np.arange(N)\n",
    "\n",
    "# Figure size\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Width of a bar \n",
    "width = 0.3       \n",
    "\n",
    "# Plotting\n",
    "plt.bar(ind, blue_bar , width, label='Train')\n",
    "plt.bar(ind + width, orange_bar, width, label='Test')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy on train vs test sets')\n",
    "\n",
    "plt.xticks(ind + width / 2, ('ExtraTreesClassifier', \n",
    "                             'ExtraTreesClassifier_Finetuned', \n",
    "                             'LogisticRegression',\n",
    "                              'MLPClassifier',\n",
    "                             'SVC',\n",
    "                             'RandomForestClassifier'),\n",
    "           rotation = 90)\n",
    "\n",
    "\n",
    "# Finding the best position for legends and putting it\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The prediction scores for the test set and the scores from the training were fairly close. The use of Cross Validation (and a high number of folds = 10) gave a good estimation of the performance on an unseen dataset.\n",
    "\n",
    "The dataset is very 'wide' with about 2 times the amount of rows/samples as there are features. We attempted two methods of feature/dimensionality reduction, RFE and PCA. RFE proved to work the best on the cross validated train data set.\n",
    "\n",
    "The best performing (in terms of accuracy) classifier on both train & test datasets was `ExtraTreesClassifier`, which seems to neither be over- or underfiting as the scores on both the training and test data are pretty close.\n",
    "\n",
    "The `LogisticRegression`, `MLPClassifier` and `SVC` classifiers all seems to be over- or underfitting slightly as their scores on the test set are a bit lower than on the training set.\n",
    "\n",
    "`RandomForestClassifier` is actually performing a bit better on the test set than on the training set.\n",
    "\n",
    "The tuned `ExtraTreesClassifier` does perform a bit better than the model with standard hyperparameters, it could perhaps be even better with some more tuning. On the other hand, more tuning could also risk overfitting it to the test set, which is why it's probably best to leave it as is."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xsm10C7KRTiq"
   ],
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml4t')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "24d583f1203bdfe50eab2a92d34cce79a7ce63c8befccd2ca229701da87b9007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
